## Web Search Agents with Local LLMs
This repository contains materials from a presentation delivered at The AI Summit, part of Asia Tech x Singapore (ATxSG) 2025. The sharing session focused on building AI agents for web search using local LLMs, demonstrating practical approaches to creating intelligent search capabilities without relying on cloud-based language models.
# Overview
The project showcases the implementation of AI agents that can:
Parse and understand natural language search queries
Perform web searches using the Tavily API
Process and synthesize search results
Present information to users in a coherent manner
# Requirements
The core dependencies for this project are listed in requirements.txt. Key components include:
LangChain and LangGraph for agent orchestration
Ollama for running local language models
Streamlit for the user interface
Tavily for web search capabilities
# Getting Started
Install the required dependencies using pip install -r requirements.txt
Make sure you have Ollama installed and running with your preferred local language model
Set up your Tavily API key for web search functionality
Run the Streamlit application to interact with the search agent
# Features
Local LLM integration - no dependency on cloud-based API services
Agent-based architecture for complex reasoning
Web search capabilities with result synthesis
Interactive user interface
The presentation slides (.ppt) included in this repository provide additional context and architectural details from the ATxSG 2025 session.
